{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78dca707",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# dataset_collector.py\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcsv\u001b[39;00m\n\u001b[32m      6\u001b[39m mp_hands = mp.solutions.hands\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mithr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\__init__.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msolutions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msolutions\u001b[39;00m \n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtasks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtasks\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mithr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mithr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtasks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio_classifier\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtasks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio_embedder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m AudioClassifier = \u001b[43maudio_classifier\u001b[49m.AudioClassifier\n\u001b[32m     22\u001b[39m AudioClassifierOptions = audio_classifier.AudioClassifierOptions\n\u001b[32m     23\u001b[39m AudioClassifierResult = audio_classifier.AudioClassifierResult\n",
      "\u001b[31mNameError\u001b[39m: name 'audio_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# dataset_collector.py\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "gesture_name = input(\"Enter the gesture name: \")\n",
    "\n",
    "data = []\n",
    "\n",
    "print(\"Collecting data... Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            landmarks.append(gesture_name)\n",
    "            data.append(landmarks)\n",
    "\n",
    "    cv2.imshow(\"Collecting Data\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save to CSV\n",
    "with open('hand_gesture_data.csv', 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"{len(data)} samples collected for '{gesture_name}' gesture.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f317cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deletion:\n",
      " 63\n",
      "screenshot      1068\n",
      "zoom_in          583\n",
      "click            545\n",
      "drag             502\n",
      "right_click      491\n",
      "move             363\n",
      "scroll_up        332\n",
      "scroll_down      325\n",
      "double_click     231\n",
      "Name: count, dtype: int64\n",
      "After deletion:\n",
      " 63\n",
      "screenshot      1068\n",
      "zoom_in          583\n",
      "click            545\n",
      "drag             502\n",
      "right_click      491\n",
      "move             363\n",
      "scroll_up        332\n",
      "scroll_down      325\n",
      "double_click     231\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file (skip malformed lines)\n",
    "data = pd.read_csv(\"hand_gesture_data.csv\", header=None, on_bad_lines='skip')\n",
    "\n",
    "# Print original label counts\n",
    "print(\"Before deletion:\\n\", data.iloc[:, -1].value_counts())\n",
    "\n",
    "# Label to delete\n",
    "label_to_delete = \"\"  # replace this with your target label\n",
    "\n",
    "# Remove all rows with that label\n",
    "data = data[data.iloc[:, -1] != label_to_delete]\n",
    "\n",
    "# Print new label counts\n",
    "print(\"After deletion:\\n\", data.iloc[:, -1].value_counts())\n",
    "\n",
    "# Optional: Save the cleaned dataset\n",
    "data.to_csv(\"hand_gesture_data.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ea768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts:\n",
      " 63\n",
      "screenshot      1068\n",
      "zoom_in          583\n",
      "click            545\n",
      "zoom_out         517\n",
      "drag             502\n",
      "right_click      491\n",
      "move             363\n",
      "scroll_up        332\n",
      "scroll_down      325\n",
      "double_click     231\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"hand_gesture_data.csv\", header=None, on_bad_lines='skip')  # <- skips malformed lines\n",
    "labels = data.iloc[:, -1]\n",
    "print(\"Label counts:\\n\", labels.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d434fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model Accuracy: 99.09%\n",
      "\n",
      "ðŸ“Š Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       click       0.99      0.98      0.99       109\n",
      "double_click       1.00      1.00      1.00        46\n",
      "        drag       0.99      1.00      1.00       100\n",
      "        move       0.96      0.99      0.97        73\n",
      " right_click       0.97      0.98      0.97        98\n",
      "  screenshot       1.00      1.00      1.00       214\n",
      " scroll_down       1.00      1.00      1.00        65\n",
      "   scroll_up       1.00      0.98      0.99        66\n",
      "     zoom_in       1.00      0.98      0.99       117\n",
      "    zoom_out       1.00      1.00      1.00       104\n",
      "\n",
      "    accuracy                           0.99       992\n",
      "   macro avg       0.99      0.99      0.99       992\n",
      "weighted avg       0.99      0.99      0.99       992\n",
      "\n",
      "ðŸŽ‰ Model and encoder saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"hand_gesture_data.csv\", header=None)\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Encode gesture labels\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"âœ… Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nðŸ“Š Classification Report:\\n\", classification_report(y_test, y_pred, target_names=encoder.classes_))\n",
    "\n",
    "# Save model and encoder\n",
    "joblib.dump(model, 'gesture_rf_model.pkl')\n",
    "joblib.dump(encoder, 'gesture_label_encoder.pkl')\n",
    "\n",
    "print(\"ðŸŽ‰ Model and encoder saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991aca0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# After fitting\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_pred = \u001b[43mmodel\u001b[49m.predict(X_train)\n\u001b[32m      3\u001b[39m train_acc = accuracy_score(y_train, train_pred)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ§  Training Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# After fitting\n",
    "train_pred = model.predict(X_train)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "print(f\"ðŸ§  Training Accuracy: {train_acc * 100:.2f}%\")\n",
    "\n",
    "test_pred = model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "print(f\"ðŸ§ª Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2674f57d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mthreading\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Thread\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyautogui\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mithr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\__init__.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msolutions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msolutions\u001b[39;00m \n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtasks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtasks\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m framework\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m gpu\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mithr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\tasks\\python\\__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2022 The MediaPipe Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"MediaPipe Tasks API.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m components\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mithr\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\tasks\\python\\audio\\__init__.py:21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtasks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio_classifier\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmediapipe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtasks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio_embedder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m AudioClassifier = \u001b[43maudio_classifier\u001b[49m.AudioClassifier\n\u001b[32m     22\u001b[39m AudioClassifierOptions = audio_classifier.AudioClassifierOptions\n\u001b[32m     23\u001b[39m AudioClassifierResult = audio_classifier.AudioClassifierResult\n",
      "\u001b[31mNameError\u001b[39m: name 'audio_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from threading import Thread\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import joblib\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from pynput.mouse import Controller, Button\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "# Load model and encoder\n",
    "model = joblib.load(\"gesture_rf_model.pkl\")\n",
    "encoder = joblib.load(\"gesture_label_encoder.pkl\")\n",
    "\n",
    "# Setup mouse and screen\n",
    "mouse = Controller()\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Volume control\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "minVol, maxVol = volume.GetVolumeRange()[0], volume.GetVolumeRange()[1]\n",
    "\n",
    "# Mediapipe setup\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Globals\n",
    "running = False\n",
    "mode = \"mouse\"\n",
    "current_gesture = \"None\"\n",
    "prev_x, prev_y = 0, 0\n",
    "screenshot_dir = \"screenshots\"\n",
    "os.makedirs(screenshot_dir, exist_ok=True)  # Create folder\n",
    "\n",
    "# Gesture functions\n",
    "def get_angle(a, b, c):\n",
    "    ba = np.array([a.x - b.x, a.y - b.y])\n",
    "    bc = np.array([c.x - b.x, c.y - b.y])\n",
    "    cosine = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "    return np.degrees(np.arccos(np.clip(cosine, -1.0, 1.0)))\n",
    "\n",
    "def get_distance(a, b):\n",
    "    return math.hypot(a.x - b.x, a.y - b.y)\n",
    "\n",
    "def classify_gesture(lm):\n",
    "    features = [v for pt in lm for v in (pt.x, pt.y, pt.z)]\n",
    "    features += [\n",
    "        get_angle(lm[5], lm[6], lm[8]),\n",
    "        get_angle(lm[9], lm[10], lm[12]),\n",
    "        get_distance(lm[4], lm[8])\n",
    "    ]\n",
    "    if len(features) != model.n_features_in_:\n",
    "        return \"Unknown\"\n",
    "    return encoder.inverse_transform([model.predict([features])[0]])[0]\n",
    "\n",
    "def move_mouse_smooth(tip):\n",
    "    global prev_x, prev_y\n",
    "    x, y = int(tip.x * screen_width), int(tip.y * screen_height)\n",
    "    dist = math.hypot(x - prev_x, y - prev_y)\n",
    "    smooth = 1 if dist > 60 else 3\n",
    "    curr_x = prev_x + (x - prev_x) // smooth\n",
    "    curr_y = prev_y + (y - prev_y) // smooth\n",
    "    pyautogui.moveTo(curr_x, curr_y)\n",
    "    prev_x, prev_y = curr_x, curr_y\n",
    "\n",
    "# Real-time gesture detection loop\n",
    "def gesture_loop():\n",
    "    global running, mode, current_gesture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    pTime = time.time()\n",
    "\n",
    "    while running:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                lm = hand_landmarks.landmark\n",
    "                gesture = classify_gesture(lm)\n",
    "                current_gesture = gesture\n",
    "\n",
    "                if gesture == \"screenshot\":\n",
    "                    filename = f\"screenshot_{int(time.time())}.png\"\n",
    "                    pyautogui.screenshot(os.path.join(screenshot_dir, filename))\n",
    "                    cv2.putText(frame, \"Screenshot Taken\", (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)\n",
    "\n",
    "                if mode == \"mouse\":\n",
    "                    move_mouse_smooth(lm[8])\n",
    "                elif mode == \"volume\":\n",
    "                    x1, y1 = int(lm[4].x * frame.shape[1]), int(lm[4].y * frame.shape[0])\n",
    "                    x2, y2 = int(lm[8].x * frame.shape[1]), int(lm[8].y * frame.shape[0])\n",
    "                    length = math.hypot(x2 - x1, y2 - y1)\n",
    "                    vol = np.interp(length, [50, 300], [minVol, maxVol])\n",
    "                    volume.SetMasterVolumeLevel(vol, None)\n",
    "\n",
    "        cTime = time.time()\n",
    "        fps = 1 / (cTime - pTime + 1e-6)\n",
    "        pTime = cTime\n",
    "        cv2.putText(frame, f'FPS: {int(fps)}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "        cv2.imshow(\"Gesture Control\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    running = False\n",
    "\n",
    "# GUI control functions\n",
    "def start_gesture_control():\n",
    "    global running\n",
    "    if not running:\n",
    "        running = True\n",
    "        Thread(target=gesture_loop, daemon=True).start()\n",
    "        update_labels()\n",
    "\n",
    "def stop_gesture_control():\n",
    "    global running\n",
    "    running = False\n",
    "\n",
    "def update_labels():\n",
    "    if running:\n",
    "        lbl_status.config(text=\"Running\")\n",
    "        lbl_mode.config(text=f\"Mode: {mode.upper()}\")\n",
    "        lbl_gesture.config(text=f\"Gesture: {current_gesture}\")\n",
    "        root.after(200, update_labels)\n",
    "    else:\n",
    "        lbl_status.config(text=\"Stopped\")\n",
    "\n",
    "def switch_mode():\n",
    "    global mode\n",
    "    mode = \"volume\" if mode == \"mouse\" else \"mouse\"\n",
    "\n",
    "# --- Tkinter GUI Layout ---\n",
    "root = tk.Tk()\n",
    "root.title(\"Gesture Control GUI\")\n",
    "root.geometry(\"400x300\")\n",
    "root.configure(bg=\"#222\")\n",
    "\n",
    "tk.Label(root, text=\"Gesture Control System\", font=(\"Helvetica\", 16, \"bold\"), bg=\"#222\", fg=\"white\").pack(pady=10)\n",
    "lbl_status = tk.Label(root, text=\"Stopped\", font=(\"Helvetica\", 14), bg=\"#222\", fg=\"red\")\n",
    "lbl_status.pack()\n",
    "\n",
    "lbl_mode = tk.Label(root, text=\"Mode: MOUSE\", font=(\"Helvetica\", 12), bg=\"#222\", fg=\"white\")\n",
    "lbl_mode.pack(pady=5)\n",
    "\n",
    "lbl_gesture = tk.Label(root, text=\"Gesture: None\", font=(\"Helvetica\", 12), bg=\"#222\", fg=\"lightgreen\")\n",
    "lbl_gesture.pack(pady=5)\n",
    "\n",
    "tk.Button(root, text=\"Start\", command=start_gesture_control, width=10, bg=\"green\", fg=\"white\", font=(\"Helvetica\", 12)).pack(pady=10)\n",
    "tk.Button(root, text=\"Stop\", command=stop_gesture_control, width=10, bg=\"red\", fg=\"white\", font=(\"Helvetica\", 12)).pack(pady=5)\n",
    "tk.Button(root, text=\"Switch Mode\", command=switch_mode, width=15, bg=\"blue\", fg=\"white\", font=(\"Helvetica\", 11)).pack(pady=10)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4167f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
